{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone Fish Age Estimation with Support Vector Machines\n",
    "\n",
    "In this project, I build a support vector machine classifier and regression models to predict the age of an Abalone fish based on their physical measurements. The age of the fish is denoted by the number of rings on this fish. I start by cleaning and preparing the data set, then I take my best guess for the hyperparameters and kernel to use in the support vector classifier (SVC). Next, I'll test out different kernels and hyper parameters to fine tune the SVC. \n",
    "\n",
    "I'm curious about how the results will be affected if I keep the rings variable continuous and employ a support vector regression (SVR). I'll take a similar approach as I did to the SVC and evaluate the two models by comparing their recall, precision, and f-measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, clean, and prepare data\n",
    "\n",
    "To begin, I import the Abalone fish data and take a look at the first five rows and the summary statistics of the dataset. In conjunction with the dataset description, I can see that the sex variable is made up of three different categorical variables (male, female, and infant). I start by one-hot encoding this column to generate three additional binary columns to denote the sex of the fish. Next, in order to use the SVC, the target variable needs to be binary. I encode the rings variable based on the average number of rings in the dataset (about 10 rings). Therefore, I can predict whether the fish is above or below the average age of the population using the SVC. If the fish is above 10 rings, the data is encoded as a 1 and if the fish is below 10 rings, it's encoded as a 0.\n",
    "\n",
    "Now that the data has been prepared for the SVC, I split the dataset into testing and training subsets to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shuck_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole_weight  shuck_weight  viscera_weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140        0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255        0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770        0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160        0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050        0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import fish data and column names\n",
    "import pandas as pd\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\n",
    "\n",
    "cols = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shuck_weight', 'viscera_weight',\n",
    "       'shell_weight', 'rings']\n",
    "\n",
    "fish = pd.read_csv(data_url, names=cols)  # read in data, set column names, and set NAs to NaNs\n",
    "\n",
    "fish.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shuck_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>9.933684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>3.224169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>2.825500</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length     diameter       height  whole_weight  shuck_weight  \\\n",
       "count  4177.000000  4177.000000  4177.000000   4177.000000   4177.000000   \n",
       "mean      0.523992     0.407881     0.139516      0.828742      0.359367   \n",
       "std       0.120093     0.099240     0.041827      0.490389      0.221963   \n",
       "min       0.075000     0.055000     0.000000      0.002000      0.001000   \n",
       "25%       0.450000     0.350000     0.115000      0.441500      0.186000   \n",
       "50%       0.545000     0.425000     0.140000      0.799500      0.336000   \n",
       "75%       0.615000     0.480000     0.165000      1.153000      0.502000   \n",
       "max       0.815000     0.650000     1.130000      2.825500      1.488000   \n",
       "\n",
       "       viscera_weight  shell_weight        rings  \n",
       "count     4177.000000   4177.000000  4177.000000  \n",
       "mean         0.180594      0.238831     9.933684  \n",
       "std          0.109614      0.139203     3.224169  \n",
       "min          0.000500      0.001500     1.000000  \n",
       "25%          0.093500      0.130000     8.000000  \n",
       "50%          0.171000      0.234000     9.000000  \n",
       "75%          0.253000      0.329000    11.000000  \n",
       "max          0.760000      1.005000    29.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the sex column\n",
    "encode = pd.get_dummies(fish['sex'])\n",
    "encoded_fish = pd.concat([fish, encode], axis = 1)\n",
    "# drop the now redundant sex column\n",
    "encoded_fish = encoded_fish.drop(['sex'], axis = 1)\n",
    "# rename the encoded columns to be more descriptive\n",
    "encoded_fish = encoded_fish.rename(columns = {'M':'male',\n",
    "                                              'F':'female',\n",
    "                                              'I':'infant'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the rings data so that 1 represents if the rings are above the average and 0 if below average\n",
    "encoded_fish.loc[:, 'above_avg_age'] = (encoded_fish.loc[:, 'rings'] >= 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and the targets for classification\n",
    "X = encoded_fish.drop(['rings', 'above_avg_age'], axis = 1)\n",
    "Y = encoded_fish['above_avg_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC\n",
    "\n",
    "Prior to instantiating the SVC, I took a guess at a few hyper parameter values. The cost parameter represents the level of penalty to give to the error term, in this case I selected 0.9 which is quite stringent. This means that I'm imposing a higher cost for misclassifications and employing what is called a \"hard margin\". If the cost value was lower, it would allow the model more leeway and provide a \"soft margin\". Soft margins tend to be more general and have a lower sensitvity for noise. The gamma parameter defines how influential the feature differences are in the prediction. I set gamma to be equal to 5 to start.\n",
    "\n",
    "After instantiating the model and fitting to the training data, I obtained the predicted target from the test features and compared the predictions to the test target. Using the classification report, we can evaluate how the model performed by looking at the precision, recall, and f1-score. I will be primarily looking at the f1-score since it combines precision and recall. This model performed reasonably well with an f1-score of 0.76. I'll try to improve this score by tweaking the hyperparameters and trying other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a best guess at the hyper parameters to use\n",
    "cost = .9 # penalty parameter of the error term\n",
    "gamma = 5 # defines the influence of input vectors on the margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76       416\n",
      "           1       0.76      0.76      0.76       420\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       836\n",
      "   macro avg       0.76      0.76      0.76       836\n",
      "weighted avg       0.76      0.76      0.76       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# test a LinearSVC with the initial hyper parameters\n",
    "clf1 = svm.LinearSVC(C=cost).fit(X_train, y_train)\n",
    "clf1.predict(X_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(clf1.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing other kernels in the SVC\n",
    "\n",
    "There is a \"kernel trick\" you can employ when fine tuning SVCs. Ideally you want to use a linear kernel when you data is linear. However, if you have data that is too non-linear, you can transform your input variables  so that the shape of your dataset becomes more linear. To do this you can select different kernels to transform your dataset. \n",
    "\n",
    "In this case, I tried the rbf and poly kernels to see if they perform better than the linear kernel above. Based on the f1-score, both the rbf and poly kernels perform slightly better than the linear kernel with f1-scores of 0.77 and 0.78, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75       368\n",
      "           1       0.83      0.74      0.78       468\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       836\n",
      "   macro avg       0.77      0.77      0.77       836\n",
      "weighted avg       0.77      0.77      0.77       836\n",
      "\n",
      "poly\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       393\n",
      "           1       0.80      0.76      0.78       443\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       836\n",
      "   macro avg       0.78      0.78      0.78       836\n",
      "weighted avg       0.78      0.78      0.78       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test linear, rbf and poly kernels\n",
    "for k in ('rbf', 'poly'):\n",
    "    clf = svm.SVC(gamma=gamma, kernel=k, C=cost).fit(X_train, y_train)\n",
    "    clf.predict(X_test)\n",
    "    print(k)\n",
    "    print(classification_report(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning the SVC hyper parameters\n",
    "\n",
    "I decided to move forward with the poly kernel to try and tweak some of the other hyper parameters in hopes of improving the model further. I updated the cost hyper parameter to be a bit more lienient (0.6) since we are only predicting if a fish is above or below the average age we don't need an extremely hard margin for our SVC. However, the f1-score does not change much even when reducing the cost hyper parameter. It seems like the best model in this case is an SVC with a poly kernel and a cost of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78       400\n",
      "           1       0.80      0.77      0.79       436\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       836\n",
      "   macro avg       0.78      0.78      0.78       836\n",
      "weighted avg       0.78      0.78      0.78       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# poly\n",
    "poly = svm.SVC(gamma=gamma, kernel='poly', C=0.6).fit(X_train, y_train)\n",
    "poly.predict(X_test)\n",
    "print(\"poly\")\n",
    "print(classification_report(poly.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "I was curious to see how well a support vector regression (SVR) model would predict the number of rings of the Abalone fish if we kept the number of rings continuous. I defined new targets and features for the SVR, the features were actually the same but I made a new variable for the regression for consistency. The target is now the continuous rings column that denotes the age of the Abalone fish. Next, I split the dataset into training and testing subsets.\n",
    "\n",
    "I instantiated the SVR with a poly kernel and the same hyper parameters I tuned above in the SVC. To evaluate the SVR, I predict the Abalone rings based on the test features and then compare the predictions to the actual target values in the test dataset. The following metrics are employed to understand how well the model is performing: R-squared value, mean squared error, and the variance. \n",
    "\n",
    "The R-squared value is pretty low, only 0.53, meaning that the model does not correlate the features with the number of rings very well. The mean squared error measures the average difference in error between the predicted values and the actual values. In this case, the mean squared error is about 5, ideally we'd want this value to be closer to 0 but it's still quite low which is a good sign. The variance score describes how far the actual values differ from the predicted values mean. This SVR has a variance score of 0.54 which is quite good, since we want a low variance to show that our predicted values are not deviating too far from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and the targets for regression\n",
    "X_reg = encoded_fish.drop(['rings', 'above_avg_age'], axis = 1)\n",
    "Y_reg = encoded_fish['rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into a training and testing set\n",
    "X_regtrain, X_regtest, y_regtrain, y_regtest = train_test_split(X_reg, Y_reg, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr results:\n",
      "R-squared:  0.5279085685031117\n",
      "Mean Squared Error:  4.963693231922028\n",
      "Variance:  0.5364710785202282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "svr = svm.SVR(gamma = gamma, kernel = 'poly', C=0.8).fit(X_regtrain, y_regtrain)\n",
    "pred = svr.predict(X_regtest)\n",
    "print(\"svr results:\")\n",
    "print(\"R-squared: \", svr.score(X_regtest, y_regtest))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_regtest, pred))\n",
    "print(\"Variance: \", explained_variance_score(y_regtest, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this assignment I built a SVC and a SVR to predict the number of rings an Abalone fish has (the number of rings represent their age). The SVC was employed to predict whether the fish had rings greater than or less than the average number of rings in the dataset (10 rings). I fine tuned the hyper parameters and tried out a few different kernels to get the best model. I was curious to see how an SVR would perform in predicting the number of rings as a continuous variable. My findings and observations for each step are listed below:\n",
    "\n",
    "Clean and prepare data:\n",
    "* To start, I one-hot encoded the \"sex\" column to create binary columns that hold the gender of each fish as either a male, female, or infant. \n",
    "* Next, I binned the \"rings\" column to a new column called \"above_avg_rings\". This column held a 1 where the number of rings was greater than 10 and a 0 where the number of rings was less than 10. \n",
    "* To prepare the data for modeling, I split the dataset into training and test subsets.\n",
    "\n",
    "SVC:\n",
    "* Prior to instantiating my first SVC, I set the cost and gamma hyper parameters. I set the cost hyper parameter to 0.9 which is quite stringent and therefore set a hard margin right off of the bat. This means that there is a greater penalty with misclassifications. I set the gamma hyper parameter equal to 5, this value defines how influential the feature differences are in the model. This first SVC used the linear kernel and the f1-score was 0.76, a pretty good score but it's possible that fine tuning the hyper parameters will make it better.\n",
    "* I started by trying out a few different kernels. If a linear kernel doesn't work as well because you're data is fairly non-linear you can try other kernels, such as, \"rbf\" or \"poly\" kernels to can transform your features to be more linear and improve the model. The results were slightly better with the rbf and poly kernels with f1-scores of 0.77 and 0.78, respectively.\n",
    "* I decided to move forward with the poly kernel since it had the highest f1-score. I tweaked the cost hyper parameter to be a bit more relaxed since we are just predicting whether a fish has greater than or less than the average number of rings in the dataset. This means that there will not be as large of a penalty for misclassifications. However, even with a cost of 0.6, the f1-score didn't change. Therefore, I concluded that the best SVC is a model with the poly kernel, cost = 0.9, and gamma = 5.\n",
    "\n",
    "SVR:\n",
    "* To prepare the data for an SVR, I selected the same features as the SVC but this time used the continuous \"rings\" column as the target.\n",
    "* I fit an SVR with the poly kernel and set the cost = 0.8 to predict the number of rings an Abalone fish has based on their physical attributes.\n",
    "* I evaluated the model by quantifiying the R-squared, the mean squared error, and the variance. The R-squared was pretty weak with a value of 0.53 but the mean squared error and variance were relatively low. \n",
    "\n",
    "Final Conclusions:\n",
    "* Based on my findings, it's pretty hard to predict the number of rings of an Abalone fish based on their physical attributes. However, the best approach I identified was using an SVC with a poly kernel to predict whether the number of rings of the fish was above or below the average of the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
